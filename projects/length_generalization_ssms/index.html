<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Comparing Mamba, Attention and their Hybrids | Chinmaya Kausik </title> <meta name="author" content="Chinmaya Kausik"> <meta name="description" content="Studying length generalization in Mamba, attention-based transformers, and hybrids of both for associative recall and related tasks."> <meta name="keywords" content="reinforcement learning, bandits, sequential decision-making, AI, ML, machine learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/website_icon.jpg?36d439f631f0622eaaee531b206bdfa6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chinmaya-kausik.github.io/projects/length_generalization_ssms/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Chinmaya Kausik </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">Papers </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks%20and%20teaching/">Talks and Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/personal/">Personal </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Comparing Mamba, Attention and their Hybrids</h1> <p class="post-description">Studying length generalization in Mamba, attention-based transformers, and hybrids of both for associative recall and related tasks.</p> </header> <article> <p>Link to repository <a href="https://github.com/Chinmaya-Kausik/mamba_attention_length_generalization" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="introduction">Introduction</h3> <ol> <li>Attention-based transformer models use a data-dependent averaging of features at every layer to decide which features to “pay more attention to.” This is in contrast to most other popular deep learning architectures, where features are aggregated at each layer in a data-independent way, determined by the weights of the model.</li> <li>However, the downside of this data dependent averaging is that one has to fix the maximum length or size of the input beforehand. This is undesirable for time-series data, where we want to be flexible about the length of the “context,” i.e. the input time series.</li> <li>An emerging alternative is state space models (SSMs), which model time series as dynamical systems with “internal state variables.” Many successful architectures have been introduced in this regard, such as S4 and Mamba.</li> <li>Since the main benefit is in flexibility of length of input, it is interesting to consider how these might generalize over lengths. That is, one would like to know the performance of the model at test inputs with different lengths than the training inputs.</li> <li>Three natural tasks are considered: <ol> <li>Induction head: For a specific token A, learn to find the previous occurence of A, find the token B that came after that occurence and return B. We are building a 2 layer transformer model for the induction task.</li> <li>Associative recall: Same task as the induction head task except that we need the model to learn to recall the next token for <strong>multiple</strong> query tokens A.</li> <li>Associative recall with delay: Same task as associative recall, but instead of the next token, we need to find the next token with gap or <em>delay</em> tau.</li> </ol> </li> </ol> <h3 id="what-i-did">What I did</h3> <ol> <li>I explored length generalization across various attention and Mamba based architectures. Namely, I considered: <ol> <li>No PE: Attention without positional embeddings and 2 layers</li> <li>Learned PE: Attention with positional embeddings and 2 layers</li> <li>Rotary PE: Attention with rotary positional embeddings and 2 layers</li> <li>Mamba: Mamba with 2 blocks</li> <li>Hybrid A: One Mamba block and one attention layer (without positional embeddings)</li> <li>Hybrid B: One Mamba block followed by one attention layer (without positional embeddings) followed by one Mamba block</li> </ol> </li> <li>I wrote code implementing Mamba, following existing popular implementations, as well as drew from libraries implementing both.</li> <li>Drew the following conclusions from results, essentially saying that No PE &lt; Learned PE &lt; Rotary PE &lt; Mamba &lt; Hybrid A ~ Hybrid B: <ol> <li>No PE: Using no positional embeddings shows almost no length generalization and even has imperfect performance for the train context length (32)</li> <li>Learned PE: Using learned absolute positional embeddings improves over this, since it is able to learned task and data-oriented positional embeddings and add them to the features. This however shows poor generalization since they are absolute positional embeddings.</li> <li>Rotary PE: Even better are rotary positional embeddings, which are relative position embeddings and thus only rely on the query, key and the distance between them. Such embeddings can continue to function at unseen lengths since they are relative, and thus show better length generalization.</li> <li>Mamba: Mamba performs better than any of the positional embeddings with a transformer, since state space models use dynamical systems to model sequences, and thus can inherently handle relative positions better. They are thus better at length generalization than transformers, which have to learn attention weights for fixed context lengths. However, Mamba is unable to perform perfectly at context lengths significantly far from the original.</li> <li>Hybrid-A: Mamba misses the “data-dependent averaging” ability of attention, which impedes the “selective focus” needed for memory and recall tasks, so adding an attention block allows it to improve upon this ability and Hybrid-A performs better than Mamba.</li> <li>Hybrid-B: We get a similar performance with Hybrid-B, which merely adds another Mamba layer after Hybrid-A.</li> </ol> </li> <li>The difference in performance between mamba/hybrids and purely attention-based architectures is even more stark when we introduce delay.</li> </ol> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chinmaya Kausik. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: September 26, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=document.querySelector(".navbar-collapse");e.classList.contains("show")&&e.classList.remove("show"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-papers",title:"Papers",description:"A list of my preprints and publications",section:"Navigation",handler:()=>{window.location.href="/papers/"}},{id:"nav-projects",title:"Projects",description:"Past unpublished projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-talks-and-teaching",title:"Talks and Teaching",description:"Talks given and courses taught.",section:"Navigation",handler:()=>{window.location.href="/talks%20and%20teaching/"}},{id:"nav-personal",title:"Personal",description:"Other stuff about me.",section:"Navigation",handler:()=>{window.location.href="/personal/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.location.href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/"}},{id:"news-monsoon-math-is-gearing-up-for-2022-email-me-if-you-have-ideas-about-making-it-more-effective-and-impactful",title:"Monsoon Math is gearing up for 2022! Email me if you have ideas about making it more effective and impactful.",description:"",section:"News"},{id:"news-monsoon-math-has-unfortunately-been-cancelled-for-summer-2022",title:"Monsoon Math has unfortunately been cancelled for summer 2022.",description:"",section:"News"},{id:"news-my-paper-with-prof-stefan-friedl-and-jose-pedro-quintanilha-on-an-algorithm-for-generalized-seifert-matrices-has-been-accepted-by-the-journal-of-knot-theory-and-its-ramifications",title:"My paper with Prof. Stefan Friedl and Jose Pedro Quintanilha on an algorithm for generalized Seifert matrices has been accepted by the Journal of Knot Theory and its Ramifications!",description:"",section:"News"},{id:"news-invited-to-attend-the-princeton-ml-theory-summer-school-2023",title:"Invited to attend the Princeton ML theory Summer School, 2023!",description:"",section:"News"},{id:"news-my-paper-on-learning-mixtures-of-markov-chains-and-mdps-with-kevin-tan-and-my-advisor-prof-tewari-has-been-accepted-to-icml-2023-oral",title:"My paper on learning mixtures of Markov chains and MDPs with Kevin Tan and my advisor, Prof. Tewari, has been accepted to ICML 2023 (Oral)!",description:"",section:"News"},{id:"news-two-new-preprints-confounded-rl-and-double-descent-phenomena-with-input-noise-added-to-arxiv",title:"Two new preprints (confounded RL and double descent phenomena with input noise) added to arXiv!",description:"",section:"News"},{id:"news-our-paper-denoising-low-rank-data-under-distribution-shift-double-descent-and-data-augmentation-has-been-accepted-to-the-neurips-workshop-on-the-mathematics-of-modern-machine-learning-m3l",title:"Our paper \u201cDenoising Low-Rank Data Under Distribution Shift: Double Descent and Data Augmentation\u201d has been accepted to the NeurIPS workshop on the Mathematics of Modern Machine Learning (M3L)!",description:"",section:"News"},{id:"news-i-have-received-the-rackham-international-student-fellowship-which-is-offered-to-25-students-across-graduate-departments-under-rackham",title:"I have received the Rackham International Student Fellowship, which is offered to 25 students across graduate departments under Rackham!",description:"",section:"News"},{id:"news-announcing-two-paper-acceptances-my-paper-on-offline-reinforcement-learning-in-the-presence-of-confounding-written-with-kevin-tan-yangyi-lu-maggie-makar-yixin-wang-and-my-advisor-ambuj-tewari-has-been-accepted-to-aistats-2024-my-paper-on-double-descent-phenomena-in-denoising-with-rishi-sonthalia-and-kashvi-srivastava-has-been-accepted-to-tmlr-2024",title:"Announcing two paper acceptances! My paper on offline reinforcement learning in the presence of confounding, written with Kevin Tan, Yangyi Lu, Maggie Makar, Yixin Wang and my advisor Ambuj Tewari has been accepted to AISTATS 2024. My paper on double descent phenomena in denoising with Rishi Sonthalia and Kashvi Srivastava has been accepted to TMLR 2024.",description:"",section:"News"},{id:"news-i-have-started-my-internship-at-microsoft-ads-working-on-ad-monetization-under-my-manager-ajith-moparthi-and-with-my-mentor-yannis-exarchos-excited-to-dive-into-designing-a-low-latency-update-algorithm-for-autobidding-models",title:"I have started my internship at Microsoft Ads, working on ad monetization under my manager Ajith Moparthi and with my mentor Yannis Exarchos! Excited to dive into designing a low latency update algorithm for autobidding models.",description:"",section:"News"},{id:"posts-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/"}},{id:"posts-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.location.href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"}},{id:"projects-idris",title:"idris",description:"Interactive proofs in Idris, 2019.",section:"Projects",handler:()=>{window.location.href="/projects/idris_proofs/"}},{id:"projects-in-context-learning-in-llms-for-simple-functions",title:"In-Context Learning in LLMs for Simple Functions",description:"Conducted a deeper examination of in-context learning for simple functions using small LLMs. Builds off of experiments by Garg et al.",section:"Projects",handler:()=>{window.location.href="/projects/in-context-learning/"}},{id:"projects-learning-mixtures-of-linear-dynamical-systems",title:"Learning Mixtures of Linear Dynamical Systems",description:"Designed, implemented and reproduced experiments for unsupervised clustering of trajectories drawn from linear dynamical systems.",section:"Projects",handler:()=>{window.location.href="/projects/learning_lds/"}},{id:"projects-comparing-mamba-attention-and-their-hybrids",title:"Comparing Mamba, Attention and their Hybrids",description:"Studying length generalization in Mamba, attention-based transformers, and hybrids of both for associative recall and related tasks.",section:"Projects",handler:()=>{window.location.href="/projects/length_generalization_ssms/"}},{id:"projects-updating-autobidding-models-in-near-real-time",title:"Updating Autobidding Models in Near-Real Time",description:"Designed and implemented a new algorithm for making hourly updates to models used for autobidding in ad auctions. Worked with multitask Gaussian processes.",section:"Projects",handler:()=>{window.location.href="/projects/multitask_gps/"}},{id:"projects-py-knots",title:"py_knots",description:"Knot algorithms in Python, 2022.",section:"Projects",handler:()=>{window.location.href="/projects/py_knots/"}},{id:"projects-rlhf-for-very-small-llms",title:"RLHF for Very Small LLMs",description:"Compared various RLHF methods for summarization using GPT-2 small. Implemented trainers from scratch in JAX.",section:"Projects",handler:()=>{window.location.href="/projects/rlhf_comparison/"}},{id:"projects-superficial",title:"superficial",description:"Algorithms for surfaces in Scala, 2020.",section:"Projects",handler:()=>{window.location.href="/projects/superficial/"}},{id:"projects-topological-data-analysis",title:"topological data analysis",description:"Analysing UNGA votes, 1945-2015.",section:"Projects",handler:()=>{window.location.href="/projects/tda/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%6B%61%75%73%69%6B@%75%6D%69%63%68.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=FHO3DH8AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Chinmaya-Kausik","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/chinmaya-kausik","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/ChinmayaKausik","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>