---
layout: page
title: Learning Mixtures of Linear Dynamical Systems
description: A project on unsupervised clustering of trajectories drawn from linear dynamical systems
img: assets/img/chen-and-poor.png
importance: 1
category: graduate
---

### Introduction
* Time series data is ubiquitous in most machine learning domains - financial market data, user behavior in shopping/entertainment apps, robotics, etc. Unfortunately, representing time-series data using raw observations leads to very high-dimensional, hard to deal with data.
* So, learning low-dimensional or generally parsimonious representations of time-series data is regarded as an important problem. One reasonable assumption often made to achieve this is that the time series trajectories are generated by a fixed but unknown set of models. That is, each trajectory is generated under one of the models in this unknown set. This is known as a mixture model.
* A popular class of time series models is a linear dynamical system - the next datapoint is a linear transformation of the previous datapoint, with some noise added. This is the "natural linear model" for time series, much like how linear regression works under the "natural linear model" for scalar data. The parameters of the linear transformation and the noise define a given linear dynamical system model.
* [This paper](https://arxiv.org/abs/2211.09403) by Chen and Poor provides a principled method for clustering time series trajectories from linear dynamical systems based on whether they come from the same model, only using the raw trajectory data and without previous knowledge of the underlying set of models governing the dataset.
* This is a hard chicken-and-egg problem - if one knew the clusters of trajectories coming from the same model, one could learn the underlying set of models. If one knew the set of models, one could determine clusters easily. Doing both without knowing either is a hard task solved by this paper, for which they essentially develop a principled dimensionality reduction method to reduce the dimension of the time series data before performing usual clustering.

### What I did
* This was a final team project for the graduate machine learning course - I chose this problem because of how mathematically and empirically rich it was.
* I explained the theoretical intuition to my teammates
* Wrote about a third of the code.
* I found important discrepancies between theory and experiments (especially in regard to the numerical linear algebra choices made) and corresponded with the authors about it.
* I designed some new theory-informed heuristics to determine hyperparameters like the number of models underlying the dataset, which is also a priori unknown in practice. We then implemented them and had satisfactory results, recovering the initial number of models chosen by the authors in their experiments.
* We performed an ablation study to test the performance of this method against using random projections for dimensionality reduction. These are known to perform well in some scenarios, but do much worse than Chen and Poor's method in our case.

### Code
[Link to the repository.](https://github.com/Chinmaya-Kausik/learning_mixtures_lds_py)
